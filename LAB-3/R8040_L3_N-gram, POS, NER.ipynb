{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5447c3e1",
   "metadata": {},
   "source": [
    "# NAME: NEELANJAN DUTTA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5dd8b7",
   "metadata": {},
   "source": [
    "# REGISTER NUMBER: 2448040"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e921a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import brown\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import treebank\n",
    "import math\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6941e741",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to C:\\Users\\Neelanjan\n",
      "[nltk_data]     Dutta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package reuters to C:\\Users\\Neelanjan\n",
      "[nltk_data]     Dutta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Neelanjan\n",
      "[nltk_data]     Dutta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package treebank to C:\\Users\\Neelanjan\n",
      "[nltk_data]     Dutta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to C:\\Users\\Neelanjan\n",
      "[nltk_data]     Dutta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Downloading required corpora and tokenizers\n",
    "nltk.download('brown')\n",
    "nltk.download('reuters')\n",
    "nltk.download('punkt')\n",
    "nltk.download('treebank')\n",
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49388f51",
   "metadata": {},
   "source": [
    "# Q1. Estimating N-gram probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b4ccd1",
   "metadata": {},
   "source": [
    "## **(a) Generate a lookup table for all the words in a given text for unigram probability**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "493478ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a sentence to analyze unigram probabilities: The President said the job was successfully done.\n",
      "\n",
      "Unigram Count and Probability Table:\n",
      "Word\t\tCount\tProbability\n",
      "the       \t69277\t0.040256\n",
      "president \t  871\t0.000506\n",
      "said      \t25383\t0.014750\n",
      "job       \t   42\t0.000024\n",
      "was       \t 5816\t0.003380\n",
      "successfully\t   24\t0.000014\n",
      "done      \t  105\t0.000061\n"
     ]
    }
   ],
   "source": [
    "# Loading Reuters corpus for training counts\n",
    "corpus_tokens = [w.lower() for w in reuters.words()]\n",
    "unigram_counts_corpus = Counter(corpus_tokens)\n",
    "total_words_corpus = len(corpus_tokens)\n",
    "\n",
    "# User input and removing all punctuation\n",
    "test_sentence = input(\"Enter a sentence to analyze unigram probabilities: \")\n",
    "tokens = [w for w in nltk.word_tokenize(test_sentence.lower()) if w not in string.punctuation]\n",
    "\n",
    "\n",
    "# Showing unigram counts and probabilities (from corpus)\n",
    "# Unigram table\n",
    "print(\"\\nUnigram Count and Probability Table:\")\n",
    "print(\"Word\\t\\tCount\\tProbability\")\n",
    "seen = set()\n",
    "for word in tokens:\n",
    "    if word not in seen:\n",
    "        count = unigram_counts_corpus.get(word, 0)\n",
    "        prob = count / total_words_corpus if count > 0 else 0\n",
    "        print(f\"{word:10s}\\t{count:5d}\\t{prob:.6f}\")\n",
    "        seen.add(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7d8949",
   "metadata": {},
   "source": [
    "**Goal: Generate a lookup table of unigram probabilities for all words in a given sentence, using a corpus.**\n",
    "\n",
    "> **1. Corpus Loading:** The program loads the Reuters corpus from NLTK. This corpus contains thousands of news articles — ideal for economic/business-style sentences.\n",
    "\n",
    "> **2. Preprocessing the Corpus:** onverts all words in the corpus to lowercase. Removes all punctuation marks so they don't count as words. Creates a frequency count (dictionary) of how many times each word appears — using Counter.\n",
    "\n",
    "> **3. Calculating Total Tokens:** Calculates the total number of words (excluding punctuation) in the corpus — needed to compute probabilities.\n",
    "\n",
    "> **4. Taking Input Sentence:** Asks the user to enter a sentence. Tokenizes this sentence (splits into words), removes punctuation, and converts all words to lowercase.\n",
    "\n",
    "> **5. Building the Lookup Table:** For each unique word in the input sentence, Looks up its frequency (count) from the corpus. Computes its unigram probability using given formula and display the word, its count, and its probability in a nicely formatted table.\n",
    "\n",
    "**<center>P(w) = Count(w)/Total number of words in corpus</center>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b631401",
   "metadata": {},
   "source": [
    "**(b.) Using a bigram language model, calculate the probability of;** <br><br>\n",
    "**(i.)  P(Sam/am)**  <br><br>\n",
    "**(ii.) P(green/like)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b665330e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bigram Probabilities from Custom Text -:\n",
      "\n",
      "Bigram\t\tCount(w1,w2)\tCount(w1)\tP(w2|w1)\n",
      "am sam     \t    2\t\t    3\t\t0.666667\n",
      "like green   \t    3\t\t    3\t\t1.000000\n"
     ]
    }
   ],
   "source": [
    "# Defining a rich custom multi-sentence paragraph\n",
    "\n",
    "text = \"\"\"\n",
    "I am Sam. Sam is a funny man. I am not sad, I am Sam.  \n",
    "Sam likes green eggs. I like green apples. You may like green tea.  \n",
    "I do not like green cars. But Sam likes green things.\n",
    "\"\"\"\n",
    "\n",
    "# Tokenizing\n",
    "tokens = nltk.word_tokenize(text.lower())\n",
    "\n",
    "# Counting unigrams and bigrams\n",
    "unigram_counts = Counter(tokens)\n",
    "bigram_counts = Counter(ngrams(tokens, 2))\n",
    "\n",
    "# Defining and check target bigrams\n",
    "target_bigrams = [(\"am\", \"sam\"), (\"like\", \"green\")]\n",
    "\n",
    "print(\"\\nBigram Probabilities from Custom Text -:\\n\")\n",
    "print(\"Bigram\\t\\tCount(w1,w2)\\tCount(w1)\\tP(w2|w1)\")\n",
    "\n",
    "for w1, w2 in target_bigrams:\n",
    "    count_bigram = bigram_counts.get((w1, w2), 0)\n",
    "    count_w1 = unigram_counts.get(w1, 0)\n",
    "    prob = count_bigram / count_w1 if count_w1 > 0 else 0\n",
    "\n",
    "    print(f\"{w1} {w2:8s}\\t{count_bigram:5d}\\t\\t{count_w1:5d}\\t\\t{prob:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9a54bc",
   "metadata": {},
   "source": [
    "**Goal: Calculate bigram probabilities (i.e., conditional probability P(w₂ | w₁)) for target word pairs, using counts from a custom multi-sentence paragraph.**\n",
    "\n",
    "\n",
    "> 1. **Input Text Creation:** A custom paragraph with multiple short sentences is manually defined. The paragraph is crafted to include important word pairs like \"am sam\" and \"like green\", so that these bigrams appear multiple times.\n",
    "\n",
    "> 2. **Tokenization:** The entire paragraph is tokenized using nltk.word_tokenize() and converted to lowercase. This splits the paragraph into individual word tokens for analysis.\n",
    "  \n",
    "> 3. **Unigram and Bigram Counting:** The program uses Counter to:<br>\n",
    "Count unigrams (frequency of each individual word) <br>\n",
    "Count bigrams (frequency of each consecutive word pair), using nltk.ngrams(tokens, 2)\n",
    "\n",
    "> 4. **Define Target Bigrams:** The two bigrams asked in the question are explicitly listed:\n",
    "target_bigrams = [(\"am\", \"sam\"), (\"like\", \"green\")]\n",
    "\n",
    "> 5. **Compute Bigram Probabilities:** For each bigram (w₁, w₂), the program calculates the conditional probability: <br>\n",
    "\n",
    "**<center>P(w2 ∣ w1) = Count(w1, w2)/ Count(w1)</center>**\n",
    "\n",
    "**<center>That is: How likely w₂ appears immediately after w₁</center>**\n",
    "\n",
    "- **Count(w₁, w₂) → how many times the bigram appears**\n",
    "- **Count(w₁) → how many times w₁ appears overall**\n",
    "- **P(w₂ | w₁) → computed bigram probability**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ac032f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a test sentence: The government said the plan was successful.\n",
      "\n",
      "Bigram Probabilities:\n",
      "P(the | <s>) = 6888/57340 = 0.12012557\n",
      "P(government | the) = 131/69971 = 0.00187220\n",
      "P(said | government) = 1/418 = 0.00239234\n",
      "P(the | said) = 79/1961 = 0.04028557\n",
      "P(plan | the) = 38/69971 = 0.00054308\n",
      "P(was | plan) = 6/205 = 0.02926829\n",
      "P(successful | was) = 5/9815 = 0.00050942\n",
      "P(</s> | successful) = 7/95 = 0.07368421\n",
      "\n",
      "Perplexity Calculation\n",
      "Perplexity =  129.1339\n"
     ]
    }
   ],
   "source": [
    "# Prepare corpus sentence-wise with <s> and </s>\n",
    "all_sentences = brown.sents()\n",
    "processed_tokens = []\n",
    "\n",
    "for sent in all_sentences:\n",
    "    words = ['<s>'] + [w.lower() for w in sent if w not in string.punctuation] + ['</s>']\n",
    "    processed_tokens.extend(words)\n",
    "\n",
    "# Build unigram and bigram counts\n",
    "unigram_counts = Counter(processed_tokens)\n",
    "bigram_counts = Counter(ngrams(processed_tokens, 2))\n",
    "\n",
    "# User input and tokenization\n",
    "test_sentence = input(\"Enter a test sentence: \")\n",
    "test_tokens = ['<s>'] + [w.lower() for w in nltk.word_tokenize(test_sentence) if w not in string.punctuation] + ['</s>']\n",
    "test_bigrams = list(ngrams(test_tokens, 2))\n",
    "\n",
    "# Bigram probability calculation and perplexity\n",
    "print(\"\\nBigram Probabilities:\")\n",
    "log_prob_sum = 0\n",
    "zero_prob = False\n",
    "\n",
    "for i, (w1, w2) in enumerate(test_bigrams, 1):\n",
    "    count_bigram = bigram_counts.get((w1, w2), 0)\n",
    "    count_w1 = unigram_counts.get(w1, 0)\n",
    "\n",
    "    if count_bigram == 0 or count_w1 == 0:\n",
    "        print(f\"P({w2} | {w1}) = 0  (Unseen bigram! log2 = -∞)\")\n",
    "        zero_prob = True\n",
    "        break\n",
    "    else:\n",
    "        prob = count_bigram / count_w1\n",
    "        log2_prob = math.log2(prob)\n",
    "        log_prob_sum += log2_prob\n",
    "        print(f\"P({w2} | {w1}) = {count_bigram}/{count_w1} = {prob:.8f}\")\n",
    "\n",
    "# Final perplexity output\n",
    "print(\"\\nPerplexity Calculation\")\n",
    "if zero_prob:\n",
    "    print(\"Perplexity = ∞ (at least one bigram was unseen)\")\n",
    "else:\n",
    "    N = len(test_bigrams)\n",
    "    avg_log2 = log_prob_sum / N\n",
    "    perplexity = 2 ** (-avg_log2)\n",
    "    print(f\"Perplexity =  {perplexity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244ae2cc",
   "metadata": {},
   "source": [
    "**Goal: Train a bigram language model using a corpus, then compute the perplexity of a user-given sentence.**\n",
    "\n",
    "\n",
    "> **1. Corpus Loading and Preprocessing:** The program uses the Brown corpus from NLTK, which contains real English sentences across different topics.\n",
    "Each sentence is processed: <br>\n",
    "All words are converted to lowercase for consistency. <br>\n",
    "Punctuation is removed. <br> \n",
    "Special tokens \\<s> (start) and \\</s> (end) are added to every sentence to mark sentence boundaries. <br><br>\n",
    "Then it:<br>\n",
    "Builds a unigram count: how many times each word appears.<br>\n",
    "Builds a bigram count: how many times each consecutive word pair (bigram) appears.\n",
    "\n",
    "> **2. User Input (Test Sentence):** The user is asked to type a sentence (e.g., \"The government said the plan was successful\").\n",
    "The sentence is: <br>\n",
    "Tokenized (split into words), <br>\n",
    "Lowercased, <br>\n",
    "Punctuation removed,<br>\n",
    "And padded with <s> and </s> to match the corpus format. <br>\n",
    "Then a list of bigrams is generated from the input sentence.\n",
    "\n",
    "> **3. Calculate Bigram Probabilities:** For each bigram (w₁, w₂) in the input sentence:\n",
    "It looks up: <br>\n",
    "count(w₁, w₂): how often the bigram appeared in the corpus. <br>\n",
    "count(w₁): how often w₁ appeared in total. <br>\n",
    "Then calculates the bigram probability: <br>\n",
    "\n",
    "**<center>P(w2 ∣ w1) = Count(w1, w2)/ Count(w1)</center>**\n",
    " \n",
    "> If the bigram was never seen in the corpus, its probability is 0 → log₂(prob) becomes −∞ → perplexity = ∞\n",
    "\n",
    "> **4. Compute Perplexity:** If all bigrams are seen (none are zero probability): <br>\n",
    "It sums the log₂ of each bigram probability <br>\n",
    "Computes the average log₂ probability <br>\n",
    "Then applies the perplexity formula: <br>\n",
    "\n",
    "**<center>Perplexity=2^(−average log₂ probability)</center>**\n",
    " \n",
    "> - **Perplexity: A measure of how well a language model predicts a sentence.**\n",
    "> - **Lower: The sentence is more likely, more natural, or well-formed.**\n",
    "> - **Higher: The sentence is unusual, unlikely, or ungrammatical.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684b92f0",
   "metadata": {},
   "source": [
    "### Q2. POS Tagging\n",
    "- Implement any one exercise from chapter 8 of Text book to tag the word of the input text.\n",
    "- Generate HMM tagger using transition and observational probabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71452d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a sentence to POS tag: The fox jumped on the cat.\n",
      "\n",
      "Tagged Sentence\n",
      "the/DET fox/NOUN jumped/VERB on/ADP the/DET cat/NOUN ./. "
     ]
    }
   ],
   "source": [
    "# Loading tagged corpus\n",
    "tagged_sentences = treebank.tagged_sents(tagset='universal')\n",
    "\n",
    "# Counting emissions and transitions\n",
    "transition_counts = defaultdict(Counter)\n",
    "emission_counts = defaultdict(Counter)\n",
    "tag_counts = Counter()\n",
    "\n",
    "for sent in tagged_sentences:\n",
    "    prev_tag = \"<s>\"\n",
    "    for word, tag in sent:\n",
    "        word = word.lower()\n",
    "        emission_counts[tag][word] += 1\n",
    "        transition_counts[prev_tag][tag] += 1\n",
    "        tag_counts[tag] += 1\n",
    "        prev_tag = tag\n",
    "    transition_counts[prev_tag][\"</s>\"] += 1\n",
    "\n",
    "# Compute transition and emission probabilities\n",
    "def transition_prob(t1, t2):\n",
    "    return transition_counts[t1][t2] / sum(transition_counts[t1].values())\n",
    "\n",
    "def emission_prob(word, tag):\n",
    "    return emission_counts[tag][word] / tag_counts[tag] if word in emission_counts[tag] else 1e-6\n",
    "\n",
    "# Viterbi Algorithm\n",
    "def viterbi(sentence, tag_set):\n",
    "    V = [{}]\n",
    "    path = {}\n",
    "\n",
    "    sentence = [word.lower() for word in sentence]\n",
    "\n",
    "    # Initialization\n",
    "    for tag in tag_set:\n",
    "        V[0][tag] = math.log(transition_prob(\"<s>\", tag) + 1e-12) + math.log(emission_prob(sentence[0], tag) + 1e-12)\n",
    "        path[tag] = [tag]\n",
    "\n",
    "    # Dynamic Programming\n",
    "    for t in range(1, len(sentence)):\n",
    "        V.append({})\n",
    "        new_path = {}\n",
    "\n",
    "        for curr_tag in tag_set:\n",
    "            (prob, best_prev) = max(\n",
    "                (V[t-1][prev_tag] + math.log(transition_prob(prev_tag, curr_tag) + 1e-12) + math.log(emission_prob(sentence[t], curr_tag) + 1e-12), prev_tag)\n",
    "                for prev_tag in tag_set\n",
    "            )\n",
    "            V[t][curr_tag] = prob\n",
    "            new_path[curr_tag] = path[best_prev] + [curr_tag]\n",
    "\n",
    "        path = new_path\n",
    "\n",
    "    # Termination\n",
    "    n = len(sentence) - 1\n",
    "    (prob, best_tag) = max((V[n][tag], tag) for tag in tag_set)\n",
    "    return list(zip(sentence, path[best_tag]))\n",
    "\n",
    "# Run on user input\n",
    "user_input = input(\"Enter a sentence to POS tag: \")\n",
    "tokens = nltk.word_tokenize(user_input)\n",
    "\n",
    "tag_set = list(tag_counts.keys())\n",
    "tagged = viterbi(tokens, tag_set)\n",
    "\n",
    "print(\"\\nTagged Sentence\")\n",
    "for word, tag in tagged:\n",
    "    print(f\"{word}/{tag}\", end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c73bfbb",
   "metadata": {},
   "source": [
    "**Goal: Tag each word in a sentence with its correct part of speech (like noun, verb, etc.) using a model trained from real tagged text.**\n",
    "\n",
    "> **1. Load Tagged Text:** The program uses a pre-tagged dataset (Treebank corpus from NLTK). This dataset already has thousands of English sentences, where every word is labeled with its correct part of speech. <br>\n",
    "Example: \"The/DET cat/NOUN sat/VERB\"\n",
    "\n",
    "> **2. Count Word and Tag Patterns:** From this tagged data, the program learns two things: <br>\n",
    "Which words are used with which tags (like “run” is often a verb). <br>\n",
    "Which tags usually follow other tags (like a noun is often followed by a verb). <br>\n",
    "This helps the model learn patterns — for example: <br>\n",
    "“He” is usually a pronoun. <br>\n",
    "“likes” is often a verb.<br>\n",
    "“green” is usually an adjective.<br>\n",
    "\n",
    "> **3. User Input:** The user types any sentence (e.g., \"I like green apples\"). The program splits this sentence into words (called tokenization) and converts everything to lowercase.\n",
    "\n",
    "> **4. Predict Tags Using Viterbi Algorithm:** Now comes the smart part — the program tries all possible tag sequences for your sentence and picks the best possible one based on what it learned earlier. It uses a method called Viterbi algorithm, which works like: <br>\n",
    "Starting with the first word, it checks what tags are likely. Then, for each next word, it checks: <br>\n",
    "What tags are likely for the word. <br>\n",
    "What tags came before it.<br>\n",
    "It builds the most probable tag sequence for the whole sentence.\n",
    "\n",
    "> **5. Display the Tagged Sentence:** Finally, it prints the sentence with each word tagged."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2dcd2c",
   "metadata": {},
   "source": [
    "### Q3. Named Entity Recognition\n",
    "\n",
    "- Implement NER with NLTK/spaCY/Stanford NER Tagger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54abc489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John       → B-PER\n",
      "lives      → O\n",
      "in         → O\n",
      "New        → B-GPE\n",
      "York       → I-GPE\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# Load spaCy's small English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Input sentence\n",
    "sentence = \"John lives in New York\"\n",
    "\n",
    "# Run NER pipeline\n",
    "doc = nlp(sentence)\n",
    "\n",
    "# Prepare IOB output\n",
    "for token in doc:\n",
    "    ent_iob = token.ent_iob_  # IOB tag: B, I, O\n",
    "    ent_type = token.ent_type_  # Entity type: PERSON, GPE, etc.\n",
    "    \n",
    "    if ent_iob == \"O\":\n",
    "        tag = \"O\"\n",
    "    else:\n",
    "        tag = f\"{ent_iob}-{ent_type[:3].upper()}\"  # B-PER, I-LOC, etc.\n",
    "    \n",
    "    print(f\"{token.text:10s} → {tag}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc24b49",
   "metadata": {},
   "source": [
    "**Goal: Use spaCy to implement Named Entity Recognition (NER) and tag each word in the input sentence using IOB format:**\n",
    "- B-XXX: Beginning of entity type (e.g., person, location)\n",
    "- I-XXX: Inside the entity\n",
    "- O: Not part of any named entity\n",
    "\n",
    "\n",
    "> **1. Load spaCy Model:** Load the English spaCy model (en_core_web_sm) that includes pre-trained NER capabilities. This model knows how to detect person names, locations, organizations, etc.\n",
    "\n",
    "> **2. Define the Input Sentence:** A sample sentence where \"John\" is a person and \"New York\" is a location.\n",
    "\n",
    "> **3. Apply spaCy NLP Pipeline:** Tokenizes the sentence and runs Named Entity Recognition automatically.\n",
    "\n",
    "> **4. Extract and Print IOB Tags:** For each word (token): <br>\n",
    "ent_iob_ gives the IOB tag: <br>\n",
    "B: Beginning of an entity <br>\n",
    "I: Inside an entity <br>\n",
    "O: Outside <br>\n",
    "ent_type_ gives the type of entity (like PERSON, GPE).\n",
    "\n",
    "> - If it’s not an entity → tag is O\n",
    "> - If it is → format is like B-PER, I-LOC, etc.\n",
    "> - You can also map GPE → LOC manually (as GPE refers to geopolitical entities like cities).\n",
    "\n",
    "> **5. Final Output:** For the sentence \"John lives in New York\": <br>\n",
    "John       → B-PER <br>\n",
    "lives      → O <br>\n",
    "in         → O <br>\n",
    "New        → B-LOC <br>\n",
    "York       → I-LOC\n",
    "\n",
    "\n",
    "> - **What is IOB Format?** <br>\n",
    "IOB = Inside, Outside, Beginning <br>\n",
    "Commonly used in NER labeling tasks. <br>\n",
    "Helps in identifying multi-word named entities, like \"New York\": <br>\n",
    "New → B-LOC <br>\n",
    "York → I-LOC"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
