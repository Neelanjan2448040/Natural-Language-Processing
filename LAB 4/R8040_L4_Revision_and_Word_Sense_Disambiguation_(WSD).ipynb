{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11aed598",
   "metadata": {},
   "source": [
    "## <center>MDS472C: NATURAL LANGUAGE PROCESSING</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce80475c",
   "metadata": {},
   "source": [
    "## <center>LAB4: Revision and Word Sense Disambiguation (WSD) \n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7dec7b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.metrics.distance import edit_distance\n",
    "import math\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.wsd import lesk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aba7ce6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Neelanjan\n",
      "[nltk_data]     Dutta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Neelanjan Dutta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Neelanjan\n",
      "[nltk_data]     Dutta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Neelanjan\n",
      "[nltk_data]     Dutta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Neelanjan\n",
      "[nltk_data]     Dutta\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download once if not already\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebed6dbf",
   "metadata": {},
   "source": [
    "# Question 1: Consider the following two documents: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3272ce8",
   "metadata": {},
   "source": [
    "**Doc 1: I am a student, and I currently take MDS472C. I was a student in MDS331 last trimester.** <br>\n",
    "**Doc 2: I was a student. I have taken MDS472C.** <br><br>\n",
    "\n",
    "**(a.) Create a dictionary with the positional index of the document.** <br>\n",
    "**(b.)Find the positional indexes for the words ‚Äústudent‚Äù and ‚ÄúMDS472C‚Äù.** <br>\n",
    "**(c.) Find the positional indexes for any given set of words as user input.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2dabc38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Positional Index...\n",
      "\n",
      "Doc 1 Tokens: ['i', 'am', 'a', 'student', 'and', 'i', 'currently', 'take', 'mds472c', 'i', 'was', 'a', 'student', 'in', 'mds331', 'last', 'trimester']\n",
      "\n",
      "Doc 2 Tokens: ['i', 'was', 'a', 'student', 'i', 'have', 'taken', 'mds472c']\n",
      "\n",
      "Positional Index Created!\n",
      "\n",
      "======== MENU ========\n",
      "1. Show Full Positional Index\n",
      "2. Find Positions for 'student'\n",
      "3. Find Positions for 'MDS472C'\n",
      "4. Search Word(s) of Your Choice\n",
      "5. Exit\n",
      "======================\n",
      "Enter your choice (1-5): 1\n",
      "\n",
      "FULL POSITIONAL INDEX:\n",
      "a: {1: [2, 11], 2: [2]}\n",
      "am: {1: [1]}\n",
      "and: {1: [4]}\n",
      "currently: {1: [6]}\n",
      "have: {2: [5]}\n",
      "i: {1: [0, 5, 9], 2: [0, 4]}\n",
      "in: {1: [13]}\n",
      "last: {1: [15]}\n",
      "mds331: {1: [14]}\n",
      "mds472c: {1: [8], 2: [7]}\n",
      "student: {1: [3, 12], 2: [3]}\n",
      "take: {1: [7]}\n",
      "taken: {2: [6]}\n",
      "trimester: {1: [16]}\n",
      "was: {1: [10], 2: [1]}\n",
      "\n",
      "======== MENU ========\n",
      "1. Show Full Positional Index\n",
      "2. Find Positions for 'student'\n",
      "3. Find Positions for 'MDS472C'\n",
      "4. Search Word(s) of Your Choice\n",
      "5. Exit\n",
      "======================\n",
      "Enter your choice (1-5): 2\n",
      "'student' found at: {1: [3, 12], 2: [3]}\n",
      "\n",
      "======== MENU ========\n",
      "1. Show Full Positional Index\n",
      "2. Find Positions for 'student'\n",
      "3. Find Positions for 'MDS472C'\n",
      "4. Search Word(s) of Your Choice\n",
      "5. Exit\n",
      "======================\n",
      "Enter your choice (1-5): 3\n",
      "'mds472c' found at: {1: [8], 2: [7]}\n",
      "\n",
      "======== MENU ========\n",
      "1. Show Full Positional Index\n",
      "2. Find Positions for 'student'\n",
      "3. Find Positions for 'MDS472C'\n",
      "4. Search Word(s) of Your Choice\n",
      "5. Exit\n",
      "======================\n",
      "Enter your choice (1-5): 4\n",
      "Enter word(s) separated by spaces: student\n",
      "'student' found at: {1: [3, 12], 2: [3]}\n",
      "\n",
      "======== MENU ========\n",
      "1. Show Full Positional Index\n",
      "2. Find Positions for 'student'\n",
      "3. Find Positions for 'MDS472C'\n",
      "4. Search Word(s) of Your Choice\n",
      "5. Exit\n",
      "======================\n",
      "Enter your choice (1-5): 5\n",
      "Exiting.\n"
     ]
    }
   ],
   "source": [
    "# Documents\n",
    "documents = {\n",
    "    1: \"I am a student, and I currently take MDS472C. I was a student in MDS331 last trimester.\",\n",
    "    2: \"I was a student. I have taken MDS472C.\"\n",
    "}\n",
    "\n",
    "# Preprocessing and Tokenization\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9]+', ' ', text)\n",
    "    return text.split()\n",
    "\n",
    "# Build Positional Index\n",
    "def build_positional_index(docs):\n",
    "    index = defaultdict(lambda: defaultdict(list))\n",
    "    for doc_id, text in docs.items():\n",
    "        tokens = preprocess(text)\n",
    "        print(f\"\\nDoc {doc_id} Tokens: {tokens}\")\n",
    "        for pos, token in enumerate(tokens):\n",
    "            index[token][doc_id].append(pos)\n",
    "    return index\n",
    "\n",
    "# Display Index for Specific Word\n",
    "def display_index_for(word, index):\n",
    "    word = word.lower()\n",
    "    if word in index:\n",
    "        print(f\"'{word}' found at: {dict(index[word])}\")\n",
    "    else:\n",
    "        print(f\"'{word}' not found in any document.\")\n",
    "\n",
    "# Full Positional Index\n",
    "def display_full_index(index):\n",
    "    print(\"\\nFULL POSITIONAL INDEX:\")\n",
    "    for word, posting in sorted(index.items()):\n",
    "        print(f\"{word}: {dict(posting)}\")\n",
    "\n",
    "# Main Menu Loop\n",
    "def main():\n",
    "    print(\"Building Positional Index...\")\n",
    "    index = build_positional_index(documents)\n",
    "    print(\"\\nPositional Index Created!\")\n",
    "\n",
    "    while True:\n",
    "        print(\"\\n======== MENU ========\")\n",
    "        print(\"1. Show Full Positional Index\")\n",
    "        print(\"2. Find Positions for 'student'\")\n",
    "        print(\"3. Find Positions for 'MDS472C'\")\n",
    "        print(\"4. Search Word(s) of Your Choice\")\n",
    "        print(\"5. Exit\")\n",
    "        print(\"======================\")\n",
    "\n",
    "        choice = input(\"Enter your choice (1-5): \")\n",
    "\n",
    "        if choice == '1':\n",
    "            display_full_index(index)\n",
    "        elif choice == '2':\n",
    "            display_index_for('student', index)\n",
    "        elif choice == '3':\n",
    "            display_index_for('MDS472C', index)\n",
    "        elif choice == '4':\n",
    "            words = input(\"Enter word(s) separated by spaces: \").split()\n",
    "            for word in words:\n",
    "                display_index_for(word, index)\n",
    "        elif choice == '5':\n",
    "            print(\"Exiting.\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"Invalid choice. Try again.\")\n",
    "\n",
    "# Run the program\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae28ae61",
   "metadata": {},
   "source": [
    "**Goal: Efficiently index and search words based on their exact positions in a set of documents. This is useful in information retrieval systems (like search engines) where we not only care whether a word exists, but also where it occurs.** <br>\n",
    "\n",
    "**1. Documents to Index:** We start with two small example documents. Each document has text that we want to search through later.<br>\n",
    "\n",
    "> - Doc 1: \"I am a student, and I currently take MDS472C...\"\n",
    "> - Doc 2: \"I was a student. I have taken MDS472C.\"\n",
    "\n",
    "**2. Preprocessing the Text:** Each document is cleaned by: <br>\n",
    "> - Converting to lowercase.\n",
    "> - Removing all punctuation and special characters.\n",
    "> - Splitting the cleaned sentence into a list of words (tokens).\n",
    "> - Example: <br>\n",
    "> Original ‚Üí \"I am a student.\"\n",
    "> Preprocessed ‚Üí ['i', 'am', 'a', 'student']\n",
    "\n",
    "**3. Build the Positional Index:** We create a dictionary where: <br>\n",
    "> - Each word maps to a dictionary of document IDs.\n",
    "> - Each document ID maps to a list of positions (indices) where the word appears in that document.\n",
    "> - Example:\n",
    "> 'student': {1: [3, 10], 2: [3]} <br>\n",
    "> This means: <br>\n",
    "> In Doc 1, \"student\" appears at positions 3 and 10. <br>\n",
    "> In Doc 2, it appears at position 3.\n",
    "\n",
    "**4. User Interaction via Menu:** The program gives a simple menu where the user can: <br>\n",
    "> - View the full positional index.\n",
    "> - Search for a specific word (like \"student\" or \"MDS472C\").\n",
    "> - Type in any word(s) to find their positions.\n",
    "> - Exit the program.\n",
    "\n",
    "**5. Output Example:** If the user searches for \"student\", the output will show: <br>\n",
    "> 'student' found at: {1: [3, 10], 2: [3]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f5b9ba",
   "metadata": {},
   "source": [
    "# Question 2: Prepare a word matrix for the Doc 1 and Doc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1e63954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Documents and Creating Word Matrix...\n",
      "\n",
      "Doc1 Tokens: ['i', 'am', 'a', 'student', 'and', 'i', 'currently', 'take', 'mds472c', 'i', 'was', 'a', 'student', 'in', 'mds331', 'last', 'trimester']\n",
      "\n",
      "Doc2 Tokens: ['i', 'was', 'a', 'student', 'i', 'have', 'taken', 'mds472c']\n",
      "\n",
      "Vocabulary (Unique Terms): ['a', 'am', 'and', 'currently', 'have', 'i', 'in', 'last', 'mds331', 'mds472c', 'student', 'take', 'taken', 'trimester', 'was']\n",
      "\n",
      "Word Matrix Ready!\n",
      "\n",
      "======== MENU ========\n",
      "1. Show Vocabulary\n",
      "2. Show Word Matrix (Term-Document)\n",
      "3. Search Term Frequency in Documents\n",
      "4. Exit\n",
      "======================\n",
      "Enter your choice (1-4): 1\n",
      "\n",
      "Vocabulary Terms:\n",
      "‚Ä¢ a\n",
      "‚Ä¢ am\n",
      "‚Ä¢ and\n",
      "‚Ä¢ currently\n",
      "‚Ä¢ have\n",
      "‚Ä¢ i\n",
      "‚Ä¢ in\n",
      "‚Ä¢ last\n",
      "‚Ä¢ mds331\n",
      "‚Ä¢ mds472c\n",
      "‚Ä¢ student\n",
      "‚Ä¢ take\n",
      "‚Ä¢ taken\n",
      "‚Ä¢ trimester\n",
      "‚Ä¢ was\n",
      "\n",
      "======== MENU ========\n",
      "1. Show Vocabulary\n",
      "2. Show Word Matrix (Term-Document)\n",
      "3. Search Term Frequency in Documents\n",
      "4. Exit\n",
      "======================\n",
      "Enter your choice (1-4): 2\n",
      "\n",
      "TERM-DOCUMENT MATRIX (Counts):\n",
      "\n",
      "Term            Doc1       Doc2      \n",
      "-----------------------------------\n",
      "a               2          1         \n",
      "am              1          0         \n",
      "and             1          0         \n",
      "currently       1          0         \n",
      "have            0          1         \n",
      "i               3          2         \n",
      "in              1          0         \n",
      "last            1          0         \n",
      "mds331          1          0         \n",
      "mds472c         1          1         \n",
      "student         2          1         \n",
      "take            1          0         \n",
      "taken           0          1         \n",
      "trimester       1          0         \n",
      "was             1          1         \n",
      "\n",
      "======== MENU ========\n",
      "1. Show Vocabulary\n",
      "2. Show Word Matrix (Term-Document)\n",
      "3. Search Term Frequency in Documents\n",
      "4. Exit\n",
      "======================\n",
      "Enter your choice (1-4): 3\n",
      "Enter the term to search: and\n",
      "\n",
      "Frequencies of 'and':\n",
      "Doc1: 1\n",
      "Doc2: 0\n",
      "\n",
      "======== MENU ========\n",
      "1. Show Vocabulary\n",
      "2. Show Word Matrix (Term-Document)\n",
      "3. Search Term Frequency in Documents\n",
      "4. Exit\n",
      "======================\n",
      "Enter your choice (1-4): 4\n",
      "Exiting.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Input Documents\n",
    "documents = {\n",
    "    \"Doc1\": \"I am a student, and I currently take MDS472C. I was a student in MDS331 last trimester.\",\n",
    "    \"Doc2\": \"I was a student. I have taken MDS472C.\"\n",
    "}\n",
    "\n",
    "# Step 2: Preprocessing function\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9]+', ' ', text)  # remove punctuation\n",
    "    return text.split()\n",
    "\n",
    "# Step 3: Tokenize all documents\n",
    "def tokenize_documents(docs):\n",
    "    tokenized = {}\n",
    "    for doc_name, content in docs.items():\n",
    "        tokens = preprocess(content)\n",
    "        tokenized[doc_name] = tokens\n",
    "        print(f\"\\n{doc_name} Tokens: {tokens}\")\n",
    "    return tokenized\n",
    "\n",
    "# Step 4: Build vocabulary\n",
    "def build_vocabulary(tokenized_docs):\n",
    "    vocab = sorted(set(word for tokens in tokenized_docs.values() for word in tokens))\n",
    "    print(f\"\\nVocabulary (Unique Terms): {vocab}\")\n",
    "    return vocab\n",
    "\n",
    "# Step 5: Create term-document matrix\n",
    "def create_term_doc_matrix(vocab, tokenized_docs):\n",
    "    matrix = defaultdict(dict)\n",
    "    for term in vocab:\n",
    "        for doc_name, tokens in tokenized_docs.items():\n",
    "            matrix[term][doc_name] = tokens.count(term)  # frequency\n",
    "    return matrix\n",
    "\n",
    "# Step 6: Print the matrix nicely\n",
    "def display_matrix(matrix, doc_names):\n",
    "    print(\"\\nTERM-DOCUMENT MATRIX (Counts):\\n\")\n",
    "    header = [\"Term\"] + doc_names\n",
    "    print(\"{:<15} {:<10} {:<10}\".format(*header))\n",
    "    print(\"-\" * 35)\n",
    "    for term, counts in matrix.items():\n",
    "        row = [term] + [str(counts.get(doc, 0)) for doc in doc_names]\n",
    "        print(\"{:<15} {:<10} {:<10}\".format(*row))\n",
    "\n",
    "# Step 7: Main menu\n",
    "def main():\n",
    "    print(\"Processing Documents and Creating Word Matrix...\")\n",
    "    tokenized_docs = tokenize_documents(documents)\n",
    "    vocab = build_vocabulary(tokenized_docs)\n",
    "    matrix = create_term_doc_matrix(vocab, tokenized_docs)\n",
    "    print(\"\\nWord Matrix Ready!\")\n",
    "\n",
    "    while True:\n",
    "        print(\"\\n======== MENU ========\")\n",
    "        print(\"1. Show Vocabulary\")\n",
    "        print(\"2. Show Word Matrix (Term-Document)\")\n",
    "        print(\"3. Search Term Frequency in Documents\")\n",
    "        print(\"4. Exit\")\n",
    "        print(\"======================\")\n",
    "\n",
    "        choice = input(\"Enter your choice (1-4): \")\n",
    "\n",
    "        if choice == '1':\n",
    "            print(\"\\nVocabulary Terms:\")\n",
    "            for term in vocab:\n",
    "                print(f\"‚Ä¢ {term}\")\n",
    "        elif choice == '2':\n",
    "            display_matrix(matrix, list(documents.keys()))\n",
    "        elif choice == '3':\n",
    "            word = input(\"Enter the term to search: \").lower()\n",
    "            if word in matrix:\n",
    "                print(f\"\\nFrequencies of '{word}':\")\n",
    "                for doc in documents:\n",
    "                    print(f\"{doc}: {matrix[word].get(doc, 0)}\")\n",
    "            else:\n",
    "                print(f\"Term '{word}' not found in vocabulary.\")\n",
    "        elif choice == '4':\n",
    "            print(\"Exiting.\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"Invalid choice. Try again.\")\n",
    "\n",
    "# Run the program\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360d9188",
   "metadata": {},
   "source": [
    "**Goal: Create a term-document matrix, which shows how many times each word (term) appears in each document. This helps in comparing documents based on word usage, and is a basic building block for text mining and information retrieval systems.**\n",
    "\n",
    "**1. Input Documents:** <br>\n",
    "> - We start with two short documents (Doc1 and Doc2). These are simple English sentences that contain overlapping and unique words. Example: <br>\n",
    "> Doc1: Talks about being a student and mentions \"MDS472C\" and \"MDS331\". <br>\n",
    "> Doc2: Also mentions being a student and includes \"MDS472C\".\n",
    "\n",
    "**2. Preprocessing and Tokenization:** Each document is cleaned by:<br>\n",
    "> - Lowercasing all characters. <br>\n",
    "> - Removing punctuation and special characters. <br>\n",
    "> - Splitting the sentence into words (called tokens). <br>\n",
    "> - Example: <br>\n",
    "> Original ‚Üí \"I am a student, and I currently take MDS472C.\" <br>\n",
    "> Preprocessed ‚Üí ['i', 'am', 'a', 'student', 'and', 'i', 'currently', 'take', 'mds472c']\n",
    "\n",
    "**3. Building the Vocabulary:** From all the tokenized documents, the program extracts a unique set of words (i.e., vocabulary). This is the list of terms to be checked in each document. <br>\n",
    "> - Example Vocabulary: <br>\n",
    "> ['a', 'am', 'and', 'can', 'cat', 'chase', 'currently', 'dog', ...]\n",
    "\n",
    "**4. Constructing the Term-Document Matrix:** The program now creates a matrix where: <br>\n",
    "> - Each row represents a word from the vocabulary. <br>\n",
    "> - Each column shows the number of times that word appears in a document (its term frequency).\n",
    "\n",
    "**5. Interactive Menu for the User:** The program gives a simple menu where the user can: <br>\n",
    "> - View the full vocabulary list. <br>\n",
    "> - Display the complete term-document matrix. <br>\n",
    "> - Search for any word to see how many times it appears in each document. <br>\n",
    "> - Exit the program."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461c3f0e",
   "metadata": {},
   "source": [
    "# Question 3: \n",
    "**1. Collect the documents (atleast 2) to be indexed.** <br>\n",
    "**2. Tokenize the text (convert each document into a list of tokens).** <br>\n",
    "**3. Implement linguistic preprocessing (apply normalization, stemming, lemmatization, etc)** <br>\n",
    "**4. Index the document with its frequency** <br>\n",
    "**5. Sort the list (with frequency of words or sequence of words)** <br>\n",
    "**6. Calculate the edit distance of any two words from the corpus.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef06333e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Starting Preprocessing and Indexing...\n",
      "\n",
      "\n",
      "Doc1 Original: Natural language processing (NLP) is a fascinating area of artificial intelligence.\n",
      "Normalized: natural language processing nlp is a fascinating area of artificial intelligence\n",
      "Tokens: ['natural', 'language', 'processing', 'nlp', 'is', 'a', 'fascinating', 'area', 'of', 'artificial', 'intelligence']\n",
      "Stemmed: ['natur', 'languag', 'process', 'nlp', 'is', 'a', 'fascin', 'area', 'of', 'artifici', 'intellig']\n",
      "Lemmatized: ['natural', 'language', 'processing', 'nlp', 'is', 'a', 'fascinating', 'area', 'of', 'artificial', 'intelligence']\n",
      "\n",
      "Doc2 Original: Text preprocessing includes tokenization, stemming, lemmatization, and normalization.\n",
      "Normalized: text preprocessing includes tokenization stemming lemmatization and normalization\n",
      "Tokens: ['text', 'preprocessing', 'includes', 'tokenization', 'stemming', 'lemmatization', 'and', 'normalization']\n",
      "Stemmed: ['text', 'preprocess', 'includ', 'token', 'stem', 'lemmat', 'and', 'normal']\n",
      "Lemmatized: ['text', 'preprocessing', 'includes', 'tokenization', 'stemming', 'lemmatization', 'and', 'normalization']\n",
      "\n",
      "Preprocessing and Indexing Complete!\n",
      "\n",
      "======== MENU ========\n",
      "1. Show All Tokenized + Stemmed + Lemmatized Outputs\n",
      "2. Show Frequency Index (Sorted by Frequency)\n",
      "3. Show Frequency Index (Sorted Alphabetically)\n",
      "4. Calculate Edit Distance between Two Words\n",
      "5. Exit\n",
      "======================\n",
      "Enter your choice (1-5): 1\n",
      "\n",
      "Doc1:\n",
      "‚Ä¢ Tokens: ['natural', 'language', 'processing', 'nlp', 'is', 'a', 'fascinating', 'area', 'of', 'artificial', 'intelligence']\n",
      "‚Ä¢ Stemmed: ['natur', 'languag', 'process', 'nlp', 'is', 'a', 'fascin', 'area', 'of', 'artifici', 'intellig']\n",
      "‚Ä¢ Lemmatized: ['natural', 'language', 'processing', 'nlp', 'is', 'a', 'fascinating', 'area', 'of', 'artificial', 'intelligence']\n",
      "\n",
      "Doc2:\n",
      "‚Ä¢ Tokens: ['text', 'preprocessing', 'includes', 'tokenization', 'stemming', 'lemmatization', 'and', 'normalization']\n",
      "‚Ä¢ Stemmed: ['text', 'preprocess', 'includ', 'token', 'stem', 'lemmat', 'and', 'normal']\n",
      "‚Ä¢ Lemmatized: ['text', 'preprocessing', 'includes', 'tokenization', 'stemming', 'lemmatization', 'and', 'normalization']\n",
      "\n",
      "======== MENU ========\n",
      "1. Show All Tokenized + Stemmed + Lemmatized Outputs\n",
      "2. Show Frequency Index (Sorted by Frequency)\n",
      "3. Show Frequency Index (Sorted Alphabetically)\n",
      "4. Calculate Edit Distance between Two Words\n",
      "5. Exit\n",
      "======================\n",
      "Enter your choice (1-5): 2\n",
      "\n",
      "WORD FREQUENCY INDEX:\n",
      "natural: 1\n",
      "language: 1\n",
      "processing: 1\n",
      "nlp: 1\n",
      "is: 1\n",
      "a: 1\n",
      "fascinating: 1\n",
      "area: 1\n",
      "of: 1\n",
      "artificial: 1\n",
      "intelligence: 1\n",
      "text: 1\n",
      "preprocessing: 1\n",
      "includes: 1\n",
      "tokenization: 1\n",
      "stemming: 1\n",
      "lemmatization: 1\n",
      "and: 1\n",
      "normalization: 1\n",
      "\n",
      "======== MENU ========\n",
      "1. Show All Tokenized + Stemmed + Lemmatized Outputs\n",
      "2. Show Frequency Index (Sorted by Frequency)\n",
      "3. Show Frequency Index (Sorted Alphabetically)\n",
      "4. Calculate Edit Distance between Two Words\n",
      "5. Exit\n",
      "======================\n",
      "Enter your choice (1-5): 3\n",
      "\n",
      "WORD FREQUENCY INDEX:\n",
      "a: 1\n",
      "and: 1\n",
      "area: 1\n",
      "artificial: 1\n",
      "fascinating: 1\n",
      "includes: 1\n",
      "intelligence: 1\n",
      "is: 1\n",
      "language: 1\n",
      "lemmatization: 1\n",
      "natural: 1\n",
      "nlp: 1\n",
      "normalization: 1\n",
      "of: 1\n",
      "preprocessing: 1\n",
      "processing: 1\n",
      "stemming: 1\n",
      "text: 1\n",
      "tokenization: 1\n",
      "\n",
      "======== MENU ========\n",
      "1. Show All Tokenized + Stemmed + Lemmatized Outputs\n",
      "2. Show Frequency Index (Sorted by Frequency)\n",
      "3. Show Frequency Index (Sorted Alphabetically)\n",
      "4. Calculate Edit Distance between Two Words\n",
      "5. Exit\n",
      "======================\n",
      "Enter your choice (1-5): 4\n",
      "Enter first word: includes\n",
      "Enter second word: intelligence\n",
      "\n",
      "Edit Distance between 'includes' and 'intelligence': 8\n",
      "\n",
      "======== MENU ========\n",
      "1. Show All Tokenized + Stemmed + Lemmatized Outputs\n",
      "2. Show Frequency Index (Sorted by Frequency)\n",
      "3. Show Frequency Index (Sorted Alphabetically)\n",
      "4. Calculate Edit Distance between Two Words\n",
      "5. Exit\n",
      "======================\n",
      "Enter your choice (1-5): 5\n",
      "Exiting.\n"
     ]
    }
   ],
   "source": [
    "# Documents (You can replace or extend these)\n",
    "documents = {\n",
    "    \"Doc1\": \"Natural language processing (NLP) is a fascinating area of artificial intelligence.\",\n",
    "    \"Doc2\": \"Text preprocessing includes tokenization, stemming, lemmatization, and normalization.\"\n",
    "}\n",
    "\n",
    "# Preprocessing Functions\n",
    "def normalize(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "def tokenize(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def stem_tokens(tokens):\n",
    "    ps = PorterStemmer()\n",
    "    return [ps.stem(token) for token in tokens]\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "# Step 1: Full preprocessing pipeline\n",
    "def full_preprocess(docs):\n",
    "    preprocessed = {}\n",
    "    for doc_name, content in docs.items():\n",
    "        print(f\"\\n{doc_name} Original: {content}\")\n",
    "        norm = normalize(content)\n",
    "        tokens = tokenize(norm)\n",
    "        stemmed = stem_tokens(tokens)\n",
    "        lemmatized = lemmatize_tokens(tokens)\n",
    "        preprocessed[doc_name] = {\n",
    "            \"normalized\": norm,\n",
    "            \"tokens\": tokens,\n",
    "            \"stemmed\": stemmed,\n",
    "            \"lemmatized\": lemmatized\n",
    "        }\n",
    "        print(f\"Normalized: {norm}\")\n",
    "        print(f\"Tokens: {tokens}\")\n",
    "        print(f\"Stemmed: {stemmed}\")\n",
    "        print(f\"Lemmatized: {lemmatized}\")\n",
    "    return preprocessed\n",
    "\n",
    "# Step 2: Indexing word frequency\n",
    "def index_frequency(all_tokens):\n",
    "    flat_tokens = [token for tokens in all_tokens.values() for token in tokens['tokens']]\n",
    "    freq = Counter(flat_tokens)\n",
    "    return freq\n",
    "\n",
    "# Step 3: Display frequency index\n",
    "def display_freq(freq, sort_by=\"freq\"):\n",
    "    print(\"\\nWORD FREQUENCY INDEX:\")\n",
    "    if sort_by == \"freq\":\n",
    "        sorted_items = sorted(freq.items(), key=lambda x: x[1], reverse=True)\n",
    "    else:\n",
    "        sorted_items = sorted(freq.items())\n",
    "    for word, count in sorted_items:\n",
    "        print(f\"{word}: {count}\")\n",
    "\n",
    "# Step 4: Edit distance between any two words\n",
    "def compute_edit_distance(w1, w2):\n",
    "    dist = edit_distance(w1, w2)\n",
    "    print(f\"\\nEdit Distance between '{w1}' and '{w2}': {dist}\")\n",
    "\n",
    "# Menu System\n",
    "def main():\n",
    "    print(\"üîß Starting Preprocessing and Indexing...\\n\")\n",
    "    processed = full_preprocess(documents)\n",
    "    freq_index = index_frequency(processed)\n",
    "    print(\"\\nPreprocessing and Indexing Complete!\")\n",
    "\n",
    "    while True:\n",
    "        print(\"\\n======== MENU ========\")\n",
    "        print(\"1. Show All Tokenized + Stemmed + Lemmatized Outputs\")\n",
    "        print(\"2. Show Frequency Index (Sorted by Frequency)\")\n",
    "        print(\"3. Show Frequency Index (Sorted Alphabetically)\")\n",
    "        print(\"4. Calculate Edit Distance between Two Words\")\n",
    "        print(\"5. Exit\")\n",
    "        print(\"======================\")\n",
    "\n",
    "        choice = input(\"Enter your choice (1-5): \")\n",
    "\n",
    "        if choice == '1':\n",
    "            for doc, items in processed.items():\n",
    "                print(f\"\\n{doc}:\")\n",
    "                print(f\"‚Ä¢ Tokens: {items['tokens']}\")\n",
    "                print(f\"‚Ä¢ Stemmed: {items['stemmed']}\")\n",
    "                print(f\"‚Ä¢ Lemmatized: {items['lemmatized']}\")\n",
    "        elif choice == '2':\n",
    "            display_freq(freq_index, sort_by=\"freq\")\n",
    "        elif choice == '3':\n",
    "            display_freq(freq_index, sort_by=\"alpha\")\n",
    "        elif choice == '4':\n",
    "            w1 = input(\"Enter first word: \").lower()\n",
    "            w2 = input(\"Enter second word: \").lower()\n",
    "            compute_edit_distance(w1, w2)\n",
    "        elif choice == '5':\n",
    "            print(\"Exiting.\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"Invalid choice. Try again.\")\n",
    "\n",
    "# Run the program\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ea13e5",
   "metadata": {},
   "source": [
    "**Goal: Efficiently preprocess text from documents and analyze the words through stemming, lemmatization, and word frequency. Additionally, provide an option to compute the edit distance between any two words. This is useful for tasks in natural language processing like search, spell checking, or document comparison.**\n",
    "\n",
    "**1. Documents to Process:** We start with two sample documents that contain sentences related to NLP and text preprocessing. <br>\n",
    "> - Doc1: \"Natural language processing (NLP) is a fascinating area of artificial intelligence.\" <br>\n",
    "> - Doc2: \"Text preprocessing includes tokenization, stemming, lemmatization, and normalization.\" <br>\n",
    "\n",
    "**2. Preprocessing the Text: Each document goes through the following cleaning and processing steps:** <br>\n",
    "> - Normalize: Convert to lowercase and remove punctuation. <br>\n",
    "> - Tokenize: Split the cleaned sentence into individual words. <br>\n",
    "> - Stem: Reduce each word to its root form (e.g., \"running\" ‚Üí \"run\"). <br>\n",
    "> - Lemmatize: Convert each word to its dictionary (base) form (e.g., \"better\" ‚Üí \"good\"). <br>\n",
    "> - This creates multiple versions of the text that are useful for different tasks.\n",
    "\n",
    "**3. Index Word Frequencies:** From the tokenized version of the text, we count how many times each word appears across all documents.This frequency index is useful for identifying common vs. rare words in the dataset. <br>\n",
    "> - Example Output: <br>\n",
    "> WORD FREQUENCY INDEX: <br>\n",
    "> of: 2  <br>\n",
    "> processing: 1  <br>\n",
    "> nlp: 1  <br>\n",
    "> lemmatization: 1  <br>\n",
    "> ...\n",
    "\n",
    "**4. Compute Edit Distance:** The user can enter any two words (e.g., \"token\" and \"taken\"), and the program calculates the edit distance between them. This tells how many insertions, deletions, or substitutions are needed to turn one word into the other. <br>\n",
    "> - Example: <br>\n",
    "> Edit Distance between 'token' and 'taken': 2\n",
    "\n",
    "**5. User Interaction via Menu:** The program gives a simple interactive menu where the user can: <br>\n",
    "> - View tokens, stemmed forms, and lemmatized forms for all documents. <br>\n",
    "> - View the frequency index (sorted by frequency or alphabetically). <br>\n",
    "> - Compute edit distance between two words. <br>\n",
    "> - Exit the program. <br>\n",
    "> - This makes the tool flexible and interactive for quick text analysis tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d01d33",
   "metadata": {},
   "source": [
    "# Question 4:\n",
    "**Consider the two words below:** <br>\n",
    "\t**Word A: characterization** <br>\n",
    "\t**Word B: categorization** <br><br>\n",
    "\n",
    "**Using the Levenshtein Edit Distance algorithm, where each insertion, deletion, and substitution has a cost of 1, perform the following:** <br>\n",
    "**1. Construct the Dynamic Programming (DP) matrix to compute the minimum edit distance between the two words.** <br>\n",
    "**2. Trace the optimal path of operations used to transform Word A into Word B.** <br>\n",
    "**3. Clearly specify each operation (Insert, Delete, Substitute, Match) along with the characters involved.** <br>\n",
    "**4.Print the final aligned form of both words, indicating matched and edited characters.** <br>\n",
    "**5. Report: Total Minimum Edit Distance, Number of Insertions, Number of Deletions, Number of Substitutions, Number of Matches** <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd74ea33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dynamic Programming (DP) Matrix:\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "[2, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "[3, 2, 1, 2, 3, 4, 5, 6, 7, 8, 8, 9, 10, 11, 12]\n",
      "[4, 3, 2, 2, 3, 4, 5, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "[5, 4, 3, 3, 3, 4, 5, 6, 6, 7, 7, 8, 9, 10, 11]\n",
      "[6, 5, 4, 4, 4, 4, 5, 6, 7, 7, 8, 8, 9, 10, 11]\n",
      "[7, 6, 5, 4, 5, 5, 5, 6, 7, 8, 8, 8, 9, 10, 11]\n",
      "[8, 7, 6, 5, 4, 5, 6, 6, 7, 8, 9, 9, 9, 10, 11]\n",
      "[9, 8, 7, 6, 5, 5, 6, 6, 7, 8, 9, 10, 10, 10, 11]\n",
      "[10, 9, 8, 7, 6, 6, 6, 7, 6, 7, 8, 9, 10, 11, 11]\n",
      "[11, 10, 9, 8, 7, 7, 7, 7, 7, 6, 7, 8, 9, 10, 11]\n",
      "[12, 11, 10, 9, 8, 8, 8, 8, 8, 7, 6, 7, 8, 9, 10]\n",
      "[13, 12, 11, 10, 9, 9, 9, 9, 9, 8, 7, 6, 7, 8, 9]\n",
      "[14, 13, 12, 11, 10, 10, 10, 10, 9, 9, 8, 7, 6, 7, 8]\n",
      "[15, 14, 13, 12, 11, 11, 10, 11, 10, 10, 9, 8, 7, 6, 7]\n",
      "[16, 15, 14, 13, 12, 12, 11, 11, 11, 11, 10, 9, 8, 7, 6]\n",
      "\n",
      "Aligned Words:\n",
      "Word A :  characterization\n",
      "Word B :  c-atego-rization\n",
      "Opertn :  *-*ssss-********\n",
      "\n",
      "Operation Stats:\n",
      "Matches      : 10\n",
      "Substitutions: 4\n",
      "Insertions   : 0\n",
      "Deletions    : 2\n",
      "\n",
      "Total Minimum Edit Distance: 6\n"
     ]
    }
   ],
   "source": [
    "def compute_edit_distance(wordA, wordB):\n",
    "    m, n = len(wordA), len(wordB)\n",
    "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "    op = [[None] * (n + 1) for _ in range(m + 1)]\n",
    "\n",
    "    # Initialize base cases\n",
    "    for i in range(m + 1):\n",
    "        dp[i][0] = i\n",
    "        op[i][0] = 'D' if i > 0 else None\n",
    "    for j in range(n + 1):\n",
    "        dp[0][j] = j\n",
    "        op[0][j] = 'I' if j > 0 else None\n",
    "\n",
    "    # Fill DP table\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if wordA[i - 1] == wordB[j - 1]:\n",
    "                dp[i][j] = dp[i - 1][j - 1]\n",
    "                op[i][j] = 'M'  # Match\n",
    "            else:\n",
    "                choices = [\n",
    "                    (dp[i - 1][j - 1] + 1, 'S'),  # Substitute\n",
    "                    (dp[i][j - 1] + 1, 'I'),      # Insert\n",
    "                    (dp[i - 1][j] + 1, 'D')       # Delete\n",
    "                ]\n",
    "                dp[i][j], op[i][j] = min(choices)\n",
    "\n",
    "    # Trace back\n",
    "    i, j = m, n\n",
    "    aligned_A = []\n",
    "    aligned_B = []\n",
    "    operations = []\n",
    "    stats = {\"I\": 0, \"D\": 0, \"S\": 0, \"M\": 0}\n",
    "\n",
    "    while i > 0 or j > 0:\n",
    "        current_op = op[i][j]\n",
    "        stats[current_op] += 1\n",
    "        if current_op == 'M' or current_op == 'S':\n",
    "            aligned_A.append(wordA[i - 1])\n",
    "            aligned_B.append(wordB[j - 1])\n",
    "            operations.append('*' if current_op == 'M' else 's')\n",
    "            i -= 1\n",
    "            j -= 1\n",
    "        elif current_op == 'I':\n",
    "            aligned_A.append('-')\n",
    "            aligned_B.append(wordB[j - 1])\n",
    "            operations.append('+')\n",
    "            j -= 1\n",
    "        elif current_op == 'D':\n",
    "            aligned_A.append(wordA[i - 1])\n",
    "            aligned_B.append('-')\n",
    "            operations.append('-')\n",
    "            i -= 1\n",
    "\n",
    "    # Reverse to correct order\n",
    "    aligned_A.reverse()\n",
    "    aligned_B.reverse()\n",
    "    operations.reverse()\n",
    "\n",
    "    return dp, aligned_A, aligned_B, operations, stats, dp[m][n]\n",
    "\n",
    "# === Run for Word A and Word B ===\n",
    "wordA = \"characterization\"\n",
    "wordB = \"categorization\"\n",
    "\n",
    "dp_matrix, aligned_A, aligned_B, ops, stats, total_dist = compute_edit_distance(wordA, wordB)\n",
    "\n",
    "# === Print Results ===\n",
    "print(\"\\nDynamic Programming (DP) Matrix:\")\n",
    "for row in dp_matrix:\n",
    "    print(row)\n",
    "\n",
    "print(\"\\nAligned Words:\")\n",
    "print(\"Word A : \", ''.join(aligned_A))\n",
    "print(\"Word B : \", ''.join(aligned_B))\n",
    "print(\"Opertn : \", ''.join(ops))\n",
    "\n",
    "print(\"\\nOperation Stats:\")\n",
    "print(f\"Matches      : {stats['M']}\")\n",
    "print(f\"Substitutions: {stats['S']}\")\n",
    "print(f\"Insertions   : {stats['I']}\")\n",
    "print(f\"Deletions    : {stats['D']}\")\n",
    "print(f\"\\nTotal Minimum Edit Distance: {total_dist}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2090d672",
   "metadata": {},
   "source": [
    "**Goal: Efficiently compute the edit distance between two words using dynamic programming. This also includes alignment of characters, tracking operations (insertions, deletions, substitutions, matches), and visualizing the transformation from one word to another.**\n",
    "\n",
    "**1. Words to Compare:** We work with two example words. These can be changed by the user to compare any two words. <br>\n",
    "> - Example: <br>\n",
    "> Word A: \"characterization\" <br>\n",
    "> Word B: \"categorization\"\n",
    "\n",
    "**2. Compute Edit Distance with Dynamic Programming:** A 2D table (dp) is built where dp[i][j] stores the minimum edit distance between the first i characters of Word A and first j characters of Word B. We also track the operation performed: <br>\n",
    "> 'M' ‚Üí Match (no cost) <br>\n",
    "> 'S' ‚Üí Substitute <br>\n",
    "> 'I' ‚Üí Insert <br>\n",
    "> 'D' ‚Üí Delete <br>\n",
    "\n",
    "**3. Trace Back the Alignment:** After filling the dp table, we trace back from the bottom-right cell to reconstruct: <br>\n",
    "> - The aligned version of both words. <br>\n",
    "> - A sequence of operations showing how Word A becomes Word B. <br>\n",
    "> - For example: <br>\n",
    "> Word A :  chara-cterization <br>\n",
    "> Word B :  catego-rization <br>\n",
    "> Opertn :  *ss--**s********** <br>\n",
    "> Here, * indicates match, s is substitution, - is deletion or insertion.\n",
    "\n",
    "**4. Output the Statistics:** The program shows: <br>\n",
    "> - How many matches, insertions, deletions, and substitutions were made. <br>\n",
    "> - The total minimum edit distance between the two words. <br>\n",
    "\n",
    "\n",
    "**5. Output the DP Matrix:**\n",
    "> - To understand the process step-by-step, the program also prints the full DP matrix used to compute the edit distance. <br>\n",
    "> - This is useful if you're analyzing the algorithm or debugging. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0743ba",
   "metadata": {},
   "source": [
    "### Question 5:  You are provided with a small training corpus. Your task is to identify suitable POS tags. Calculate transition and emission probabilities, and then tag a test sentence using HMM principles.** <br><br>\n",
    "**Training Corpus:** <br>\n",
    "**The cat chased the rat** <br>\n",
    "**A rat can run** <br>\n",
    "**The dog can chase the cat** <br>\n",
    "**Test Case: ‚Äú The rat can chase the cat ‚Äù**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "129c3b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Transition Probabilities ---\n",
      "P(DT | <START>) = 1.0000\n",
      "P(NN | DT) = 1.0000\n",
      "P(VBD | NN) = 0.2000\n",
      "P(<STOP> | NN) = 0.4000\n",
      "P(MD | NN) = 0.4000\n",
      "P(DT | VBD) = 1.0000\n",
      "P(VB | MD) = 1.0000\n",
      "P(<STOP> | VB) = 0.5000\n",
      "P(DT | VB) = 0.5000\n",
      "\n",
      "--- Emission Probabilities ---\n",
      "P(the | DT) = 0.8000\n",
      "P(a | DT) = 0.2000\n",
      "P(cat | NN) = 0.4000\n",
      "P(rat | NN) = 0.4000\n",
      "P(dog | NN) = 0.2000\n",
      "P(chased | VBD) = 1.0000\n",
      "P(can | MD) = 1.0000\n",
      "P(run | VB) = 0.5000\n",
      "P(chase | VB) = 0.5000\n",
      "\n",
      "Final Tagged Sentence:\n",
      "the ‚Üí DT\n",
      "rat ‚Üí NN\n",
      "can ‚Üí MD\n",
      "chase ‚Üí VB\n",
      "the ‚Üí DT\n",
      "cat ‚Üí NN\n",
      "\n",
      "Most probable tag sequence: ['DT', 'NN', 'MD', 'VB', 'DT', 'NN']\n",
      "Probability of the best path: 0.004096\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Training Data (with manual POS tagging)\n",
    "training_sentences = [\n",
    "    [(\"The\", \"DT\"), (\"cat\", \"NN\"), (\"chased\", \"VBD\"), (\"the\", \"DT\"), (\"rat\", \"NN\")],\n",
    "    [(\"A\", \"DT\"), (\"rat\", \"NN\"), (\"can\", \"MD\"), (\"run\", \"VB\")],\n",
    "    [(\"The\", \"DT\"), (\"dog\", \"NN\"), (\"can\", \"MD\"), (\"chase\", \"VB\"), (\"the\", \"DT\"), (\"cat\", \"NN\")]\n",
    "]\n",
    "\n",
    "# Step 2: Count collection\n",
    "transition_counts = defaultdict(Counter)\n",
    "emission_counts = defaultdict(Counter)\n",
    "tag_counts = Counter()\n",
    "\n",
    "for sentence in training_sentences:\n",
    "    prev_tag = \"<START>\"\n",
    "    for word, tag in sentence:\n",
    "        transition_counts[prev_tag][tag] += 1\n",
    "        emission_counts[tag][word.lower()] += 1\n",
    "        tag_counts[tag] += 1\n",
    "        prev_tag = tag\n",
    "    transition_counts[prev_tag][\"<STOP>\"] += 1\n",
    "\n",
    "# Step 3: Probability computation\n",
    "def compute_probabilities(counts_dict):\n",
    "    prob_dict = {}\n",
    "    for key1 in counts_dict:\n",
    "        total = sum(counts_dict[key1].values())\n",
    "        prob_dict[key1] = {}\n",
    "        for key2 in counts_dict[key1]:\n",
    "            prob_dict[key1][key2] = counts_dict[key1][key2] / total\n",
    "    return prob_dict\n",
    "\n",
    "transition_probs = compute_probabilities(transition_counts)\n",
    "emission_probs = compute_probabilities(emission_counts)\n",
    "\n",
    "# Step 4: Print Transition Probabilities\n",
    "print(\"\\n--- Transition Probabilities ---\")\n",
    "for prev_tag in transition_probs:\n",
    "    for curr_tag in transition_probs[prev_tag]:\n",
    "        print(f\"P({curr_tag} | {prev_tag}) = {transition_probs[prev_tag][curr_tag]:.4f}\")\n",
    "\n",
    "# Step 5: Print Emission Probabilities\n",
    "print(\"\\n--- Emission Probabilities ---\")\n",
    "for tag in emission_probs:\n",
    "    for word in emission_probs[tag]:\n",
    "        print(f\"P({word} | {tag}) = {emission_probs[tag][word]:.4f}\")\n",
    "\n",
    "# Step 6: Viterbi Algorithm (Simple Probabilities)\n",
    "test_sentence = [\"the\", \"rat\", \"can\", \"chase\", \"the\", \"cat\"]\n",
    "tags = list(tag_counts.keys())\n",
    "\n",
    "viterbi = [{}]\n",
    "backpointer = [{}]\n",
    "\n",
    "# Initialize for time step 0\n",
    "for tag in tags:\n",
    "    trans_p = transition_probs[\"<START>\"].get(tag, 0.0001)\n",
    "    emit_p = emission_probs[tag].get(test_sentence[0], 0.0001)\n",
    "    viterbi[0][tag] = trans_p * emit_p\n",
    "    backpointer[0][tag] = \"<START>\"\n",
    "\n",
    "# Time step t > 0\n",
    "for t in range(1, len(test_sentence)):\n",
    "    viterbi.append({})\n",
    "    backpointer.append({})\n",
    "    for curr_tag in tags:\n",
    "        max_prob = 0\n",
    "        best_prev = None\n",
    "        for prev_tag in tags:\n",
    "            trans_p = transition_probs[prev_tag].get(curr_tag, 0.0001)\n",
    "            emit_p = emission_probs[curr_tag].get(test_sentence[t], 0.0001)\n",
    "            prob = viterbi[t-1][prev_tag] * trans_p * emit_p\n",
    "            if prob > max_prob:\n",
    "                max_prob = prob\n",
    "                best_prev = prev_tag\n",
    "        viterbi[t][curr_tag] = max_prob\n",
    "        backpointer[t][curr_tag] = best_prev\n",
    "\n",
    "# Final tag selection\n",
    "n = len(test_sentence) - 1\n",
    "(prob, final_tag) = max(\n",
    "    ((viterbi[n][tag] * transition_probs[tag].get(\"<STOP>\", 0.0001), tag) for tag in tags),\n",
    "    key=lambda x: x[0]\n",
    ")\n",
    "\n",
    "# Backtracking\n",
    "final_path = [final_tag]\n",
    "for t in range(n, 0, -1):\n",
    "    final_path.insert(0, backpointer[t][final_path[0]])\n",
    "\n",
    "# Step 7: Final Output\n",
    "print(\"\\nFinal Tagged Sentence:\")\n",
    "for word, tag in zip(test_sentence, final_path):\n",
    "    print(f\"{word} ‚Üí {tag}\")\n",
    "\n",
    "print(f\"\\nMost probable tag sequence: {final_path}\")\n",
    "print(f\"Probability of the best path: {prob:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e62ff4f",
   "metadata": {},
   "source": [
    "**Goal: Build a Hidden Markov Model (HMM) for Part-of-Speech (POS) tagging, using:** <br>\n",
    "- **Transition probabilities (between tags),**\n",
    "- **Emission probabilities (word given a tag),**\n",
    "- **And the Viterbi algorithm to determine the most likely tag sequence for a given sentence.**\n",
    "\n",
    "\n",
    "**1. Words and Tags to Train From:** We start with manually tagged training sentences. These small examples define the tag sequences the model will learn from. <br>\n",
    "> - Example training data: <br>\n",
    "> [[(\"The\", \"DT\"), (\"cat\", \"NN\"), (\"chased\", \"VBD\"), (\"the\", \"DT\"), (\"rat\", \"NN\")], [(\"A\", \"DT\"), (\"rat\", \"NN\"), (\"can\", \"MD\"), (\"run\", \"VB\")],\n",
    "  [(\"The\", \"DT\"), (\"dog\", \"NN\"), (\"can\", \"MD\"), (\"chase\", \"VB\"), (\"the\", \"DT\"), (\"cat\", \"NN\")]]\n",
    "\n",
    "**2. Count Collection:** We go through each training sentence and: <br>\n",
    "> - Count transitions from one tag to the next (e.g., DT ‚Üí NN, NN ‚Üí VBD).\n",
    "> - Count emissions of words given their tags (e.g., P(\"cat\" | NN)).\n",
    "> - Track tag frequency overall.\n",
    "> - Special tags: <br>\n",
    "> \\<START> represents the beginning of a sentence. <br>\n",
    "> \\<STOP> is added after the last word in each sentence.\n",
    "\n",
    "**3. Compute Probabilities:** From the counts: <br>\n",
    "> - Transition Probability: <br>\n",
    "> P(current_tag | previous_tag) = count(previous ‚Üí current) / total transitions from previous\n",
    "> - Emission Probability: <br>\n",
    "> P(word | tag) = count(word emitted by tag) / total emissions of tag\n",
    "\n",
    "**4. Output the Transition Probabilities:** We print the calculated values like:\n",
    "> --- Transition Probabilities --- <br>\n",
    "P(DT | <START>) = 1.0000   <br>\n",
    "P(NN | DT) = 1.0000   <br>\n",
    "P(VBD | NN) = 0.3333   <br>\n",
    "...\n",
    "\n",
    "**5. Output the Emission Probabilities:** Likewise, we show which words are most likely for each tag:\n",
    "> --- Emission Probabilities --- <br>\n",
    "P(the | DT) = 0.6000   <br>\n",
    "P(a | DT) = 0.2000   <br>\n",
    "P(cat | NN) = 0.3333   <br>\n",
    "...\n",
    "\n",
    "**6. Apply Viterbi Algorithm to Test Sentence:** We use the learned model to predict tags for a new sentence: <br>\n",
    "> - Test Input: [\"the\", \"rat\", \"can\", \"chase\", \"the\", \"cat\"] <br>\n",
    "> - Viterbi does the following: <br>\n",
    "> Uses dynamic programming to find the most probable path (sequence of tags). <br>\n",
    "> Keeps a backpointer to reconstruct the best path. <br>\n",
    "> Handles unseen transitions/emissions with a small smoothing value (e.g., 0.0001). <br>\n",
    "\n",
    "**7. Final Output: Tagged Sentence + Probability:** We display the predicted POS tags for the sentence and the total path probability.\n",
    "> - Example output:\n",
    "> Final Tagged Sentence:\n",
    "the ‚Üí DT  <br>\n",
    "rat ‚Üí NN  <br>\n",
    "can ‚Üí MD  <br>\n",
    "chase ‚Üí VB  <br>\n",
    "the ‚Üí DT   <br>\n",
    "cat ‚Üí NN   <br>\n",
    "\n",
    "> Most probable tag sequence: ['DT', 'NN', 'MD', 'VB', 'DT', 'NN'] <br>\n",
    "> Probability of the best path: 0.000032 <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947f19c5",
   "metadata": {},
   "source": [
    "# Question 6: Word Senses <br>\n",
    "**(a.) Collect a small corpus of example sentences of varying lengths from any newspaper or magazine. Using WordNet or any standard dictionary, determine how many senses there are for each of the open-class words in each sentence.** <br>\n",
    "**(b.) Implement Lesk algorithm for Word Sense Disambiguation (WSD)** <br>\n",
    "**(c.) Using WordNet or a standard reference dictionary, tag each open-class word in your corpus with its correct tag.** <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "122272dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence 1: The bank will not be open on Sunday.\n",
      "POS Tags: [('The', 'DT'), ('bank', 'NN'), ('will', 'MD'), ('not', 'RB'), ('be', 'VB'), ('open', 'JJ'), ('on', 'IN'), ('Sunday', 'NNP'), ('.', '.')]\n",
      "Word Senses and Disambiguation:\n",
      "\n",
      "üî∏ Word: bank\n",
      " - POS: n\n",
      " - Total Senses: 10\n",
      " - All Senses:\n",
      "   [1] bank.n.01 - sloping land (especially the slope beside a body of water)\n",
      "   [2] depository_financial_institution.n.01 - a financial institution that accepts deposits and channels the money into lending activities\n",
      "   [3] bank.n.03 - a long ridge or pile\n",
      "   [4] bank.n.04 - an arrangement of similar objects in a row or in tiers\n",
      "   [5] bank.n.05 - a supply or stock held in reserve for future use (especially in emergencies)\n",
      "   [6] bank.n.06 - the funds held by a gambling house or the dealer in some gambling games\n",
      "   [7] bank.n.07 - a slope in the turn of a road or track; the outside is higher than the inside in order to reduce the effects of centrifugal force\n",
      "   [8] savings_bank.n.02 - a container (usually with a slot in the top) for keeping money at home\n",
      "   [9] bank.n.09 - a building in which the business of banking transacted\n",
      "   [10] bank.n.10 - a flight maneuver; aircraft tips laterally about its longitudinal axis (especially in turning)\n",
      "Predicted Sense using Lesk: savings_bank.n.02 - a container (usually with a slot in the top) for keeping money at home\n",
      "\n",
      "üî∏ Word: open\n",
      " - POS: a\n",
      " - Total Senses: 21\n",
      " - All Senses:\n",
      "   [1] open.a.01 - affording unobstructed entrance and exit; not shut or closed\n",
      "   [2] open.a.02 - affording free passage or access\n",
      "   [3] exposed.s.01 - with no protection or shield\n",
      "   [4] open.s.04 - open to or in view of all\n",
      "   [5] open.a.05 - used of mouth or eyes\n",
      "   [6] open.s.06 - not having been filled\n",
      "   [7] open.s.07 - accessible to all\n",
      "   [8] assailable.s.01 - not defended or capable of being defended\n",
      "   [9] loose.s.09 - (of textures) full of small openings or gaps\n",
      "   [10] open.s.10 - having no protecting cover or enclosure\n",
      "   [11] open.a.11 - (set theory) of an interval that contains neither of its endpoints\n",
      "   [12] open.s.12 - not brought to a conclusion; subject to further thought\n",
      "   [13] open.s.13 - not sealed or having been unsealed\n",
      "   [14] open.s.14 - without undue constriction as from e.g. tenseness or inhibition\n",
      "   [15] receptive.a.02 - ready or willing to receive favorably\n",
      "   [16] overt.a.01 - open and observable; not secret or hidden\n",
      "   [17] open.s.17 - not requiring union membership\n",
      "   [18] capable.s.02 - possibly accepting or permitting\n",
      "   [19] clear.s.03 - affording free passage or view\n",
      "   [20] candid.s.03 - openly straightforward and direct without reserve or secretiveness\n",
      "   [21] open.s.21 - ready for business\n",
      "Predicted Sense using Lesk: overt.a.01 - open and observable; not secret or hidden\n",
      "\n",
      "üî∏ Word: Sunday\n",
      " - POS: n\n",
      " - Total Senses: 2\n",
      " - All Senses:\n",
      "   [1] sunday.n.01 - first day of the week; observed as a day of rest and worship by most Christians\n",
      "   [2] sunday.n.02 - United States evangelist (1862-1935)\n",
      "Predicted Sense using Lesk: sunday.n.02 - United States evangelist (1862-1935)\n",
      "\n",
      "Sentence 2: He went to the bank of the river to fish.\n",
      "POS Tags: [('He', 'PRP'), ('went', 'VBD'), ('to', 'TO'), ('the', 'DT'), ('bank', 'NN'), ('of', 'IN'), ('the', 'DT'), ('river', 'NN'), ('to', 'TO'), ('fish', 'VB'), ('.', '.')]\n",
      "Word Senses and Disambiguation:\n",
      "\n",
      "üî∏ Word: went\n",
      " - POS: v\n",
      " - Total Senses: 30\n",
      " - All Senses:\n",
      "   [1] travel.v.01 - change location; move, travel, or proceed, also metaphorically\n",
      "   [2] go.v.02 - follow a procedure or take a course\n",
      "   [3] go.v.03 - move away from a place into another direction\n",
      "   [4] become.v.01 - enter or assume a certain state or condition\n",
      "   [5] go.v.05 - be awarded; be allotted\n",
      "   [6] run.v.05 - have a particular form\n",
      "   [7] run.v.03 - stretch out over a distance, space, time, or scope; run or extend between two points or beyond a certain point\n",
      "   [8] proceed.v.04 - follow a certain course\n",
      "   [9] go.v.09 - be abolished or discarded\n",
      "   [10] go.v.10 - be or continue to be in a certain condition\n",
      "   [11] sound.v.02 - make a certain noise or sound\n",
      "   [12] function.v.01 - perform as expected when applied\n",
      "   [13] run_low.v.01 - to be spent or finished\n",
      "   [14] move.v.13 - progress by being changed\n",
      "   [15] survive.v.01 - continue to live through hardship or adversity\n",
      "   [16] go.v.16 - pass, fare, or elapse; of a certain state of affairs or action\n",
      "   [17] die.v.01 - pass from physical life and lose all bodily attributes and functions necessary to sustain life\n",
      "   [18] belong.v.03 - be in the right place or situation\n",
      "   [19] go.v.19 - be ranked or compare\n",
      "   [20] start.v.09 - begin or set in motion\n",
      "   [21] move.v.15 - have a turn; make one's move in a game\n",
      "   [22] go.v.22 - be contained in\n",
      "   [23] go.v.23 - be sounded, played, or expressed\n",
      "   [24] blend.v.02 - blend or harmonize\n",
      "   [25] go.v.25 - lead, extend, or afford access\n",
      "   [26] fit.v.02 - be the right size or shape; fit correctly or as desired\n",
      "   [27] rifle.v.02 - go through in search of something; search through someone's belongings in an unauthorized way\n",
      "   [28] go.v.28 - be spent\n",
      "   [29] plump.v.04 - give support (to) or make a choice (of) one out of a group or number\n",
      "   [30] fail.v.04 - stop operating or functioning\n",
      "Predicted Sense using Lesk: survive.v.01 - continue to live through hardship or adversity\n",
      "\n",
      "üî∏ Word: bank\n",
      " - POS: n\n",
      " - Total Senses: 10\n",
      " - All Senses:\n",
      "   [1] bank.n.01 - sloping land (especially the slope beside a body of water)\n",
      "   [2] depository_financial_institution.n.01 - a financial institution that accepts deposits and channels the money into lending activities\n",
      "   [3] bank.n.03 - a long ridge or pile\n",
      "   [4] bank.n.04 - an arrangement of similar objects in a row or in tiers\n",
      "   [5] bank.n.05 - a supply or stock held in reserve for future use (especially in emergencies)\n",
      "   [6] bank.n.06 - the funds held by a gambling house or the dealer in some gambling games\n",
      "   [7] bank.n.07 - a slope in the turn of a road or track; the outside is higher than the inside in order to reduce the effects of centrifugal force\n",
      "   [8] savings_bank.n.02 - a container (usually with a slot in the top) for keeping money at home\n",
      "   [9] bank.n.09 - a building in which the business of banking transacted\n",
      "   [10] bank.n.10 - a flight maneuver; aircraft tips laterally about its longitudinal axis (especially in turning)\n",
      "Predicted Sense using Lesk: bank.n.07 - a slope in the turn of a road or track; the outside is higher than the inside in order to reduce the effects of centrifugal force\n",
      "\n",
      "üî∏ Word: river\n",
      " - POS: n\n",
      " - Total Senses: 1\n",
      " - All Senses:\n",
      "   [1] river.n.01 - a large natural stream of water (larger than a creek)\n",
      "Predicted Sense using Lesk: river.n.01 - a large natural stream of water (larger than a creek)\n",
      "\n",
      "üî∏ Word: fish\n",
      " - POS: v\n",
      " - Total Senses: 2\n",
      " - All Senses:\n",
      "   [1] fish.v.01 - seek indirectly\n",
      "   [2] fish.v.02 - catch or try to catch fish or shellfish\n",
      "Predicted Sense using Lesk: fish.v.02 - catch or try to catch fish or shellfish\n",
      "\n",
      "Sentence 3: The judge sentenced the criminal to five years in prison.\n",
      "POS Tags: [('The', 'DT'), ('judge', 'NN'), ('sentenced', 'VBD'), ('the', 'DT'), ('criminal', 'NN'), ('to', 'TO'), ('five', 'CD'), ('years', 'NNS'), ('in', 'IN'), ('prison', 'NN'), ('.', '.')]\n",
      "Word Senses and Disambiguation:\n",
      "\n",
      "üî∏ Word: judge\n",
      " - POS: n\n",
      " - Total Senses: 2\n",
      " - All Senses:\n",
      "   [1] judge.n.01 - a public official authorized to decide questions brought before a court of justice\n",
      "   [2] evaluator.n.01 - an authority who is able to estimate worth or quality\n",
      "Predicted Sense using Lesk: judge.n.01 - a public official authorized to decide questions brought before a court of justice\n",
      "\n",
      "üî∏ Word: sentenced\n",
      " - POS: v\n",
      " - Total Senses: 1\n",
      " - All Senses:\n",
      "   [1] sentence.v.01 - pronounce a sentence on (somebody) in a court of law\n",
      "Predicted Sense using Lesk: sentence.v.01 - pronounce a sentence on (somebody) in a court of law\n",
      "\n",
      "üî∏ Word: criminal\n",
      " - POS: n\n",
      " - Total Senses: 1\n",
      " - All Senses:\n",
      "   [1] criminal.n.01 - someone who has committed a crime or has been legally convicted of a crime\n",
      "Predicted Sense using Lesk: criminal.n.01 - someone who has committed a crime or has been legally convicted of a crime\n",
      "\n",
      "üî∏ Word: years\n",
      " - POS: n\n",
      " - Total Senses: 7\n",
      " - All Senses:\n",
      "   [1] old_age.n.01 - a late time of life\n",
      "   [2] long_time.n.01 - a prolonged period of time\n",
      "   [3] days.n.01 - the time during which someone's life continues\n",
      "   [4] year.n.01 - a period of time containing 365 (or 366) days\n",
      "   [5] year.n.02 - a period of time occupying a regular part of a calendar year that is used for some particular activity\n",
      "   [6] year.n.03 - the period of time that it takes for a planet (as, e.g., Earth or Mars) to make a complete revolution around the sun\n",
      "   [7] class.n.06 - a body of students who graduate together\n",
      "Predicted Sense using Lesk: year.n.03 - the period of time that it takes for a planet (as, e.g., Earth or Mars) to make a complete revolution around the sun\n",
      "\n",
      "üî∏ Word: prison\n",
      " - POS: n\n",
      " - Total Senses: 2\n",
      " - All Senses:\n",
      "   [1] prison.n.01 - a correctional institution where persons are confined while on trial or for punishment\n",
      "   [2] prison.n.02 - a prisonlike situation; a place of seeming confinement\n",
      "Predicted Sense using Lesk: prison.n.02 - a prisonlike situation; a place of seeming confinement\n",
      "\n",
      "Sentence 4: The stock market crashed due to economic uncertainty.\n",
      "POS Tags: [('The', 'DT'), ('stock', 'NN'), ('market', 'NN'), ('crashed', 'VBD'), ('due', 'JJ'), ('to', 'TO'), ('economic', 'JJ'), ('uncertainty', 'NN'), ('.', '.')]\n",
      "Word Senses and Disambiguation:\n",
      "\n",
      "üî∏ Word: stock\n",
      " - POS: n\n",
      " - Total Senses: 17\n",
      " - All Senses:\n",
      "   [1] stock.n.01 - the capital raised by a corporation through the issue of shares entitling holders to an ownership interest (equity)\n",
      "   [2] stock.n.02 - the merchandise that a shop has on hand\n",
      "   [3] stock.n.03 - the handle of a handgun or the butt end of a rifle or shotgun or part of the support of a machine gun or artillery gun\n",
      "   [4] stock_certificate.n.01 - a certificate documenting the shareholder's ownership in the corporation\n",
      "   [5] store.n.02 - a supply of something available for future use\n",
      "   [6] lineage.n.01 - the descendants of one individual\n",
      "   [7] breed.n.01 - a special variety of domesticated animals within a species\n",
      "   [8] broth.n.01 - liquid in which meat and vegetables are simmered; used as a basis for e.g. soups or sauces\n",
      "   [9] stock.n.09 - the reputation and popularity a person has\n",
      "   [10] stock.n.10 - persistent thickened stem of a herbaceous perennial plant\n",
      "   [11] stock.n.11 - a plant or stem onto which a graft is made; especially a plant grown specifically to provide the root part of grafted plants\n",
      "   [12] stock.n.12 - any of several Old World plants cultivated for their brightly colored flowers\n",
      "   [13] malcolm_stock.n.01 - any of various ornamental flowering plants of the genus Malcolmia\n",
      "   [14] stock.n.14 - lumber used in the construction of something\n",
      "   [15] stock.n.15 - the handle end of some implements or tools\n",
      "   [16] neckcloth.n.01 - an ornamental white cravat\n",
      "   [17] livestock.n.01 - any animals kept for use or profit\n",
      "Predicted Sense using Lesk: stock.n.11 - a plant or stem onto which a graft is made; especially a plant grown specifically to provide the root part of grafted plants\n",
      "\n",
      "üî∏ Word: market\n",
      " - POS: n\n",
      " - Total Senses: 5\n",
      " - All Senses:\n",
      "   [1] market.n.01 - the world of commercial activity where goods and services are bought and sold\n",
      "   [2] market.n.02 - the customers for a particular product or service\n",
      "   [3] grocery_store.n.01 - a marketplace where groceries are sold\n",
      "   [4] market.n.04 - the securities markets in the aggregate\n",
      "   [5] marketplace.n.02 - an area in a town where a public mercantile establishment is set up\n",
      "Predicted Sense using Lesk: marketplace.n.02 - an area in a town where a public mercantile establishment is set up\n",
      "\n",
      "üî∏ Word: crashed\n",
      " - POS: v\n",
      " - Total Senses: 13\n",
      " - All Senses:\n",
      "   [1] crash.v.01 - fall or come down violently\n",
      "   [2] crash.v.02 - move with, or as if with, a crashing noise\n",
      "   [3] crash.v.03 - undergo damage or destruction on impact\n",
      "   [4] crash.v.04 - move violently as through a barrier\n",
      "   [5] crash.v.05 - break violently or noisily; smash\n",
      "   [6] crash.v.06 - occupy, usually uninvited\n",
      "   [7] crash.v.07 - make a sudden loud sound\n",
      "   [8] barge_in.v.01 - enter uninvited; informal\n",
      "   [9] crash.v.09 - cause to crash\n",
      "   [10] crash.v.10 - hurl or thrust violently\n",
      "   [11] crash.v.11 - undergo a sudden and severe downturn\n",
      "   [12] crash.v.12 - stop operating\n",
      "   [13] doss.v.01 - sleep in a convenient place\n",
      "Predicted Sense using Lesk: crash.v.09 - cause to crash\n",
      "\n",
      "üî∏ Word: due\n",
      " - POS: a\n",
      " - Total Senses: 4\n",
      " - All Senses:\n",
      "   [1] due.a.01 - owed and payable immediately or on demand\n",
      "   [2] due.s.02 - scheduled to arrive\n",
      "   [3] due.a.03 - suitable to or expected in the circumstances\n",
      "   [4] ascribable.s.01 - capable of being assigned or credited to\n",
      "Predicted Sense using Lesk: due.a.03 - suitable to or expected in the circumstances\n",
      "\n",
      "üî∏ Word: economic\n",
      " - POS: a\n",
      " - Total Senses: 5\n",
      " - All Senses:\n",
      "   [1] economic.a.01 - of or relating to an economy, the system of production and management of material wealth\n",
      "   [2] economic.a.02 - of or relating to the science of economics\n",
      "   [3] economic.s.03 - using the minimum of time or resources necessary for effectiveness\n",
      "   [4] economic.s.04 - concerned with worldly necessities of life (especially money)\n",
      "   [5] economic.s.05 - financially rewarding\n",
      "Predicted Sense using Lesk: economic.a.02 - of or relating to the science of economics\n",
      "\n",
      "üî∏ Word: uncertainty\n",
      " - POS: n\n",
      " - Total Senses: 2\n",
      " - All Senses:\n",
      "   [1] uncertainty.n.01 - being unsettled or in doubt or dependent on chance\n",
      "   [2] doubt.n.01 - the state of being unsure of something\n",
      "Predicted Sense using Lesk: uncertainty.n.01 - being unsettled or in doubt or dependent on chance\n"
     ]
    }
   ],
   "source": [
    "# POS tag mapping from Penn Treebank to WordNet\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Sample news/magazine sentences\n",
    "sentences = [\n",
    "    \"The bank will not be open on Sunday.\",\n",
    "    \"He went to the bank of the river to fish.\",\n",
    "    \"The judge sentenced the criminal to five years in prison.\",\n",
    "    \"The stock market crashed due to economic uncertainty.\"\n",
    "]\n",
    "\n",
    "# Stopwords to ignore\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Process each sentence\n",
    "for idx, sentence in enumerate(sentences):\n",
    "    print(f\"\\nSentence {idx + 1}: {sentence}\")\n",
    "    tokens = word_tokenize(sentence)\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    \n",
    "    print(f\"POS Tags: {pos_tags}\")\n",
    "    print(\"Word Senses and Disambiguation:\")\n",
    "\n",
    "    for word, tag in pos_tags:\n",
    "        wn_pos = get_wordnet_pos(tag)\n",
    "        if wn_pos and word.lower() not in stop_words:\n",
    "            senses = wn.synsets(word, pos=wn_pos)\n",
    "            print(f\"\\nüî∏ Word: {word}\")\n",
    "            print(f\" - POS: {wn_pos}\")\n",
    "            print(f\" - Total Senses: {len(senses)}\")\n",
    "\n",
    "            if senses:\n",
    "                print(\" - All Senses:\")\n",
    "                for i, s in enumerate(senses):\n",
    "                    print(f\"   [{i+1}] {s.name()} - {s.definition()}\")\n",
    "\n",
    "                # Lesk Disambiguation\n",
    "                best_sense = lesk(tokens, word, pos=wn_pos)\n",
    "                if best_sense:\n",
    "                    print(f\"Predicted Sense using Lesk: {best_sense.name()} - {best_sense.definition()}\")\n",
    "                else:\n",
    "                    print(\"Lesk failed to disambiguate.\")\n",
    "            else:\n",
    "                print(\"No senses found in WordNet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394bf699",
   "metadata": {},
   "source": [
    "**Goal: To analyze multiple meanings (word senses) of open-class words using WordNet, and disambiguate their correct meaning based on context using the Lesk Algorithm. This is useful in Word Sense Disambiguation (WSD), a key problem in Natural Language Processing.**\n",
    "\n",
    "**1. Collect Sentences from a Magazine/News Article:** We use 4 example sentences: <br>\n",
    "> Sentence 1: The bank will not be open on Sunday. <br>\n",
    "> Sentence 2: He went to the bank of the river to fish. <br>\n",
    "> Sentence 3: The judge sentenced the criminal to five years in prison. <br>\n",
    "> Sentence 4: The stock market crashed due to economic uncertainty. <br>\n",
    "\n",
    "**2. Preprocessing Steps:** Tokenize each sentence into words. <br>\n",
    "> - Get POS tags for each word using nltk.pos_tag. <br>\n",
    "> - Filter only open-class words: nouns, verbs, adjectives, adverbs. <br>\n",
    "> - Map POS tags to WordNet format for accurate sense lookup. <br>\n",
    "\n",
    "**3. Find Word Senses using WordNet:**\n",
    "> - For each open-class word, get all possible meanings (called synsets) from WordNet. <br>\n",
    "> - Display all senses with their definitions.\n",
    "\n",
    "**4. Apply the Lesk Algorithm:**\n",
    "> - The Lesk algorithm selects the most likely sense of a word based on overlapping words in the context. <br>\n",
    "> - It returns the best-fitting synset (meaning).\n",
    "\n",
    "**5. Output Format (Per Sentence):**\n",
    "> - Original sentence <br>\n",
    "> - Tokenized and POS-tagged words <br>\n",
    "> - For each open-class word: <br>\n",
    "> POS category <br>\n",
    "> Total senses found in WordNet <br>\n",
    "> List of all senses <br>\n",
    "> Best sense predicted by Lesk algorithm <br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
